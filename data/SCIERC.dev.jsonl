{"label": "USED-FOR", "text": "This paper presents an [[ algorithm ]] for << computing optical flow , shape , motion , lighting , and albedo >> from an image sequence of a rigidly-moving Lambertian object under distant illumination ."}
{"label": "USED-FOR", "text": "This paper presents an << algorithm >> for computing optical flow , shape , motion , lighting , and albedo from an [[ image sequence ]] of a rigidly-moving Lambertian object under distant illumination ."}
{"label": "FEATURE-OF", "text": "This paper presents an algorithm for computing optical flow , shape , motion , lighting , and albedo from an << image sequence >> of a [[ rigidly-moving Lambertian object ]] under distant illumination ."}
{"label": "FEATURE-OF", "text": "This paper presents an algorithm for computing optical flow , shape , motion , lighting , and albedo from an image sequence of a << rigidly-moving Lambertian object >> under [[ distant illumination ]] ."}
{"label": "CONJUNCTION", "text": "The problem is formulated in a manner that subsumes structure from [[ motion ]] , << multi-view stereo >> , and photo-metric stereo as special cases ."}
{"label": "CONJUNCTION", "text": "The problem is formulated in a manner that subsumes structure from motion , [[ multi-view stereo ]] , and << photo-metric stereo >> as special cases ."}
{"label": "USED-FOR", "text": "The << algorithm >> utilizes both [[ spatial and temporal intensity variation ]] as cues : the former constrains flow and the latter constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces ."}
{"label": "HYPONYM-OF", "text": "The algorithm utilizes both spatial and temporal intensity variation as << cues >> : the [[ former ]] constrains flow and the latter constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces ."}
{"label": "USED-FOR", "text": "The algorithm utilizes both spatial and temporal intensity variation as cues : the [[ former ]] constrains << flow >> and the latter constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces ."}
{"label": "CONJUNCTION", "text": "The algorithm utilizes both spatial and temporal intensity variation as cues : the [[ former ]] constrains flow and the << latter >> constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces ."}
{"label": "HYPONYM-OF", "text": "The algorithm utilizes both spatial and temporal intensity variation as << cues >> : the former constrains flow and the [[ latter ]] constrains surface orientation ; combining both cues enables dense reconstruction of both textured and texture-less surfaces ."}
{"label": "USED-FOR", "text": "The algorithm utilizes both spatial and temporal intensity variation as cues : the former constrains flow and the [[ latter ]] constrains << surface orientation >> ; combining both cues enables dense reconstruction of both textured and texture-less surfaces ."}
{"label": "USED-FOR", "text": "The algorithm utilizes both spatial and temporal intensity variation as cues : the former constrains flow and the latter constrains surface orientation ; combining both [[ cues ]] enables << dense reconstruction of both textured and texture-less surfaces >> ."}
{"label": "USED-FOR", "text": "The << algorithm >> works by iteratively [[ estimating affine camera parameters , illumination , shape , and albedo ]] in an alternating fashion ."}
{"label": "USED-FOR", "text": "An [[ entity-oriented approach ]] to << restricted-domain parsing >> is proposed ."}
{"label": "USED-FOR", "text": "Like semantic grammar , [[ this ]] allows easy exploitation of << limited domain semantics >> ."}
{"label": "USED-FOR", "text": "In addition , [[ it ]] facilitates << fragmentary recognition >> and the use of multiple parsing strategies , and so is particularly useful for robust recognition of extra-grammatical input ."}
{"label": "USED-FOR", "text": "In addition , [[ it ]] facilitates fragmentary recognition and the use of << multiple parsing strategies >> , and so is particularly useful for robust recognition of extra-grammatical input ."}
{"label": "USED-FOR", "text": "In addition , it facilitates fragmentary recognition and the use of [[ multiple parsing strategies ]] , and so is particularly useful for robust << recognition of extra-grammatical input >> ."}
{"label": "USED-FOR", "text": "Representative samples from an entity-oriented language definition are presented , along with a [[ control structure ]] for an << entity-oriented parser >> , some parsing strategies that use the control structure , and worked examples of parses ."}
{"label": "USED-FOR", "text": "Representative samples from an entity-oriented language definition are presented , along with a control structure for an entity-oriented parser , some << parsing strategies >> that use the [[ control structure ]] , and worked examples of parses ."}
{"label": "PART-OF", "text": "A << parser >> incorporating the [[ control structure ]] and the parsing strategies is currently under implementation ."}
{"label": "USED-FOR", "text": "This paper summarizes the formalism of Category Cooccurrence Restrictions -LRB- CCRs -RRB- and describes two [[ parsing algorithms ]] that interpret << it >> ."}
{"label": "FEATURE-OF", "text": "The use of CCRs leads to << syntactic descriptions >> formulated entirely with [[ restrictive statements ]] ."}
{"label": "USED-FOR", "text": "The paper shows how conventional [[ algorithms ]] for the analysis of context free languages can be adapted to the << CCR formalism >> ."}
{"label": "USED-FOR", "text": "The paper shows how conventional << algorithms >> for the analysis of [[ context free languages ]] can be adapted to the CCR formalism ."}
{"label": "FEATURE-OF", "text": "Special attention is given to the part of the parser that checks the fulfillment of [[ logical well-formedness conditions ]] on << trees >> ."}
{"label": "USED-FOR", "text": "We present a [[ text mining method ]] for finding << synonymous expressions >> based on the distributional hypothesis in a set of coherent corpora ."}
{"label": "USED-FOR", "text": "We present a << text mining method >> for finding synonymous expressions based on the [[ distributional hypothesis ]] in a set of coherent corpora ."}
{"label": "EVALUATE-FOR", "text": "This paper proposes a new methodology to improve the [[ accuracy ]] of a << term aggregation system >> using each author 's text as a coherent corpus ."}
{"label": "EVALUATE-FOR", "text": "This paper proposes a new << methodology >> to improve the accuracy of a [[ term aggregation system ]] using each author 's text as a coherent corpus ."}
{"label": "EVALUATE-FOR", "text": "Our proposed method improves the [[ accuracy ]] of our << term aggregation system >> , showing that our approach is successful ."}
{"label": "EVALUATE-FOR", "text": "Our proposed << method >> improves the accuracy of our [[ term aggregation system ]] , showing that our approach is successful ."}
{"label": "USED-FOR", "text": "In this work , we present a [[ technique ]] for << robust estimation >> , which by explicitly incorporating the inherent uncertainty of the estimation procedure , results in a more efficient robust estimation algorithm ."}
{"label": "USED-FOR", "text": "In this work , we present a [[ technique ]] for robust estimation , which by explicitly incorporating the inherent uncertainty of the estimation procedure , results in a more << efficient robust estimation algorithm >> ."}
{"label": "USED-FOR", "text": "In this work , we present a << technique >> for robust estimation , which by explicitly incorporating the [[ inherent uncertainty of the estimation procedure ]] , results in a more efficient robust estimation algorithm ."}
{"label": "USED-FOR", "text": "The combination of these two [[ strategies ]] results in a << robust estimation procedure >> that provides a significant speed-up over existing RANSAC techniques , while requiring no prior information to guide the sampling process ."}
{"label": "COMPARE", "text": "The combination of these two strategies results in a << robust estimation procedure >> that provides a significant speed-up over existing [[ RANSAC techniques ]] , while requiring no prior information to guide the sampling process ."}
{"label": "COMPARE", "text": "In particular , our [[ algorithm ]] requires , on average , 3-10 times fewer samples than standard << RANSAC >> , which is in close agreement with theoretical predictions ."}
{"label": "EVALUATE-FOR", "text": "The efficiency of the << algorithm >> is demonstrated on a selection of [[ geometric estimation problems ]] ."}
{"label": "HYPONYM-OF", "text": "An attempt has been made to use an [[ Augmented Transition Network ]] as a procedural << dialog model >> ."}
{"label": "USED-FOR", "text": "The development of such a model appears to be important in several respects : as a << device >> to represent and to use different [[ dialog schemata ]] proposed in empirical conversation analysis ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs ."}
{"label": "USED-FOR", "text": "The development of such a model appears to be important in several respects : as a device to represent and to use different [[ dialog schemata ]] proposed in empirical << conversation analysis >> ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs ."}
{"label": "USED-FOR", "text": "The development of such a model appears to be important in several respects : as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a << device >> to represent and to use [[ models ]] of verbal interaction ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs ."}
{"label": "USED-FOR", "text": "The development of such a model appears to be important in several respects : as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a device to represent and to use [[ models ]] of << verbal interaction >> ; as a device combining knowledge about dialog schemata and about verbal interaction with knowledge about task-oriented and goal-directed dialogs ."}
{"label": "CONJUNCTION", "text": "The development of such a model appears to be important in several respects : as a device to represent and to use different dialog schemata proposed in empirical conversation analysis ; as a device to represent and to use models of verbal interaction ; as a device combining knowledge about [[ dialog schemata ]] and about << verbal interaction >> with knowledge about task-oriented and goal-directed dialogs ."}
{"label": "USED-FOR", "text": "A standard [[ ATN ]] should be further developed in order to account for the << verbal interactions >> of task-oriented dialogs ."}
{"label": "FEATURE-OF", "text": "A standard ATN should be further developed in order to account for the [[ verbal interactions ]] of << task-oriented dialogs >> ."}
{"label": "USED-FOR", "text": "We present a practically [[ unsupervised learning method ]] to produce << single-snippet answers >> to definition questions in question answering systems that supplement Web search engines ."}
{"label": "USED-FOR", "text": "We present a practically unsupervised learning method to produce single-snippet answers to definition questions in [[ question answering systems ]] that supplement << Web search engines >> ."}
{"label": "USED-FOR", "text": "The [[ method ]] exploits << on-line encyclopedias and dictionaries >> to generate automatically an arbitrarily large number of positive and negative definition examples , which are then used to train an svm to separate the two classes ."}
{"label": "USED-FOR", "text": "The method exploits [[ on-line encyclopedias and dictionaries ]] to generate automatically an arbitrarily large number of << positive and negative definition examples >> , which are then used to train an svm to separate the two classes ."}
{"label": "USED-FOR", "text": "The method exploits on-line encyclopedias and dictionaries to generate automatically an arbitrarily large number of [[ positive and negative definition examples ]] , which are then used to train an << svm >> to separate the two classes ."}
{"label": "COMPARE", "text": "We show experimentally that the proposed method is viable , that [[ it ]] outperforms the << alternative >> of training the system on questions and news articles from trec , and that it helps the search engine handle definition questions significantly better ."}
{"label": "USED-FOR", "text": "We show experimentally that the proposed method is viable , that it outperforms the alternative of training the << system >> on questions and [[ news articles ]] from trec , and that it helps the search engine handle definition questions significantly better ."}
{"label": "PART-OF", "text": "We show experimentally that the proposed method is viable , that it outperforms the alternative of training the system on questions and [[ news articles ]] from << trec >> , and that it helps the search engine handle definition questions significantly better ."}
{"label": "USED-FOR", "text": "We show experimentally that the proposed method is viable , that it outperforms the alternative of training the system on questions and news articles from trec , and that [[ it ]] helps the << search engine >> handle definition questions significantly better ."}
{"label": "USED-FOR", "text": "We revisit the << classical decision-theoretic problem of weighted expert voting >> from a [[ statistical learning perspective ]] ."}
{"label": "USED-FOR", "text": "In the case of known expert competence levels , we give [[ sharp error estimates ]] for the << optimal rule >> ."}
{"label": "USED-FOR", "text": "We analyze a [[ reweighted version of the Kikuchi approximation ]] for estimating the << log partition function of a product distribution >> defined over a region graph ."}
{"label": "FEATURE-OF", "text": "We analyze a reweighted version of the Kikuchi approximation for estimating the [[ log partition function of a product distribution ]] defined over a << region graph >> ."}
{"label": "FEATURE-OF", "text": "We establish sufficient conditions for the [[ concavity ]] of our << reweighted objective function >> in terms of weight assignments in the Kikuchi expansion , and show that a reweighted version of the sum product algorithm applied to the Kikuchi region graph will produce global optima of the Kikuchi approximation whenever the algorithm converges ."}
{"label": "USED-FOR", "text": "We establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the Kikuchi expansion , and show that a [[ reweighted version of the sum product algorithm ]] applied to the << Kikuchi region graph >> will produce global optima of the Kikuchi approximation whenever the algorithm converges ."}
{"label": "FEATURE-OF", "text": "We establish sufficient conditions for the concavity of our reweighted objective function in terms of weight assignments in the Kikuchi expansion , and show that a reweighted version of the sum product algorithm applied to the Kikuchi region graph will produce [[ global optima ]] of the << Kikuchi approximation >> whenever the algorithm converges ."}
{"label": "FEATURE-OF", "text": "Finally , we provide an explicit characterization of the polytope of concavity in terms of the [[ cycle structure ]] of the << region graph >> ."}
{"label": "USED-FOR", "text": "We apply a [[ decision tree based approach ]] to << pronoun resolution >> in spoken dialogue ."}
{"label": "USED-FOR", "text": "We apply a decision tree based approach to [[ pronoun resolution ]] in << spoken dialogue >> ."}
{"label": "USED-FOR", "text": "Our [[ system ]] deals with << pronouns >> with NP - and non-NP-antecedents ."}
{"label": "USED-FOR", "text": "Our system deals with << pronouns >> with [[ NP - and non-NP-antecedents ]] ."}
{"label": "USED-FOR", "text": "We present a set of [[ features ]] designed for << pronoun resolution >> in spoken dialogue and determine the most promising features ."}
{"label": "USED-FOR", "text": "We present a set of features designed for [[ pronoun resolution ]] in << spoken dialogue >> and determine the most promising features ."}
{"label": "EVALUATE-FOR", "text": "We evaluate the << system >> on twenty [[ Switchboard dialogues ]] and show that it compares well to Byron 's -LRB- 2002 -RRB- manually tuned system ."}
{"label": "COMPARE", "text": "We evaluate the system on twenty Switchboard dialogues and show that [[ it ]] compares well to << Byron 's -LRB- 2002 -RRB- manually tuned system >> ."}
{"label": "USED-FOR", "text": "We present a new [[ approach ]] for building an efficient and robust << classifier >> for the two class problem , that localizes objects that may appear in the image under different orien-tations ."}
{"label": "USED-FOR", "text": "We present a new approach for building an efficient and robust [[ classifier ]] for the two << class problem >> , that localizes objects that may appear in the image under different orien-tations ."}
{"label": "PART-OF", "text": "In contrast to other works that address this problem using multiple classifiers , each one specialized for a specific orientation , we propose a simple two-step << approach >> with an [[ estimation stage ]] and a classification stage ."}
{"label": "CONJUNCTION", "text": "In contrast to other works that address this problem using multiple classifiers , each one specialized for a specific orientation , we propose a simple two-step approach with an [[ estimation stage ]] and a << classification stage >> ."}
{"label": "PART-OF", "text": "In contrast to other works that address this problem using multiple classifiers , each one specialized for a specific orientation , we propose a simple two-step << approach >> with an estimation stage and a [[ classification stage ]] ."}
{"label": "USED-FOR", "text": "The estimator yields an initial set of potential << object poses >> that are then validated by the [[ classifier ]] ."}
{"label": "EVALUATE-FOR", "text": "This methodology allows reducing the [[ time complexity ]] of the << algorithm >> while classification results remain high ."}
{"label": "USED-FOR", "text": "The << classifier >> we use in both stages is based on a [[ boosted combination of Random Ferns ]] over local histograms of oriented gradients -LRB- HOGs -RRB- , which we compute during a pre-processing step ."}
{"label": "FEATURE-OF", "text": "The classifier we use in both stages is based on a << boosted combination of Random Ferns >> over [[ local histograms of oriented gradients -LRB- HOGs -RRB- ]] , which we compute during a pre-processing step ."}
{"label": "USED-FOR", "text": "The classifier we use in both stages is based on a boosted combination of Random Ferns over << local histograms of oriented gradients -LRB- HOGs -RRB- >> , which we compute during a [[ pre-processing step ]] ."}
{"label": "USED-FOR", "text": "Both the use of [[ supervised learning ]] and working on the gradient space makes our << approach >> robust while being efficient at run-time ."}
{"label": "USED-FOR", "text": "Both the use of supervised learning and working on the [[ gradient space ]] makes our << approach >> robust while being efficient at run-time ."}
{"label": "FEATURE-OF", "text": "We show these properties by thorough testing on standard databases and on a new << database >> made of [[ motorbikes under planar rotations ]] , and with challenging conditions such as cluttered backgrounds , changing illumination conditions and partial occlusions ."}
{"label": "FEATURE-OF", "text": "We show these properties by thorough testing on standard databases and on a new << database >> made of motorbikes under planar rotations , and with challenging [[ conditions ]] such as cluttered backgrounds , changing illumination conditions and partial occlusions ."}
{"label": "HYPONYM-OF", "text": "We show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging << conditions >> such as [[ cluttered backgrounds ]] , changing illumination conditions and partial occlusions ."}
{"label": "CONJUNCTION", "text": "We show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging conditions such as [[ cluttered backgrounds ]] , << changing illumination conditions >> and partial occlusions ."}
{"label": "HYPONYM-OF", "text": "We show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging << conditions >> such as cluttered backgrounds , [[ changing illumination conditions ]] and partial occlusions ."}
{"label": "CONJUNCTION", "text": "We show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging conditions such as cluttered backgrounds , [[ changing illumination conditions ]] and << partial occlusions >> ."}
{"label": "HYPONYM-OF", "text": "We show these properties by thorough testing on standard databases and on a new database made of motorbikes under planar rotations , and with challenging << conditions >> such as cluttered backgrounds , changing illumination conditions and [[ partial occlusions ]] ."}
{"label": "USED-FOR", "text": "A very simple improved [[ duration model ]] has reduced the error rate by about 10 % in both << triphone and semiphone systems >> ."}
{"label": "EVALUATE-FOR", "text": "A very simple improved duration model has reduced the [[ error rate ]] by about 10 % in both << triphone and semiphone systems >> ."}
{"label": "USED-FOR", "text": "A new << training strategy >> has been tested which , by itself , did not provide useful improvements but suggests that improvements can be obtained by a related [[ rapid adaptation technique ]] ."}
{"label": "USED-FOR", "text": "Finally , the << recognizer >> has been modified to use [[ bigram back-off language models ]] ."}
{"label": "USED-FOR", "text": "The [[ system ]] was then transferred from the << RM task >> to the ATIS CSR task and a limited number of development tests performed ."}
{"label": "USED-FOR", "text": "The [[ system ]] was then transferred from the RM task to the << ATIS CSR task >> and a limited number of development tests performed ."}
{"label": "CONJUNCTION", "text": "The system was then transferred from the [[ RM task ]] to the << ATIS CSR task >> and a limited number of development tests performed ."}
{"label": "USED-FOR", "text": "A new [[ approach ]] for << Interactive Machine Translation >> where the author interacts during the creation or the modification of the document is proposed ."}
{"label": "USED-FOR", "text": "This paper presents a new << interactive disambiguation scheme >> based on the [[ paraphrasing ]] of a parser 's multiple output ."}
{"label": "USED-FOR", "text": "We describe a novel [[ approach ]] to << statistical machine translation >> that combines syntactic information in the source language with recent advances in phrasal translation ."}
{"label": "PART-OF", "text": "We describe a novel << approach >> to statistical machine translation that combines [[ syntactic information ]] in the source language with recent advances in phrasal translation ."}
{"label": "CONJUNCTION", "text": "We describe a novel approach to statistical machine translation that combines [[ syntactic information ]] in the source language with recent advances in << phrasal translation >> ."}
{"label": "PART-OF", "text": "We describe a novel << approach >> to statistical machine translation that combines syntactic information in the source language with recent advances in [[ phrasal translation ]] ."}
{"label": "USED-FOR", "text": "This << method >> requires a [[ source-language dependency parser ]] , target language word segmentation and an unsupervised word alignment component ."}
{"label": "CONJUNCTION", "text": "This method requires a [[ source-language dependency parser ]] , << target language word segmentation >> and an unsupervised word alignment component ."}
{"label": "USED-FOR", "text": "This << method >> requires a source-language dependency parser , [[ target language word segmentation ]] and an unsupervised word alignment component ."}
{"label": "CONJUNCTION", "text": "This method requires a source-language dependency parser , [[ target language word segmentation ]] and an << unsupervised word alignment component >> ."}
{"label": "USED-FOR", "text": "This << method >> requires a source-language dependency parser , target language word segmentation and an [[ unsupervised word alignment component ]] ."}
{"label": "CONJUNCTION", "text": "We describe an efficient decoder and show that using these [[ tree-based models ]] in combination with conventional << SMT models >> provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser ."}
{"label": "USED-FOR", "text": "We describe an efficient decoder and show that using these [[ tree-based models ]] in combination with conventional SMT models provides a promising << approach >> that incorporates the power of phrasal SMT with the linguistic generality available in a parser ."}
{"label": "USED-FOR", "text": "We describe an efficient decoder and show that using these tree-based models in combination with conventional [[ SMT models ]] provides a promising << approach >> that incorporates the power of phrasal SMT with the linguistic generality available in a parser ."}
{"label": "CONJUNCTION", "text": "We describe an efficient decoder and show that using these tree-based models in combination with conventional SMT models provides a promising approach that incorporates the power of [[ phrasal SMT ]] with the << linguistic generality >> available in a parser ."}
{"label": "USED-FOR", "text": "We describe an efficient decoder and show that using these tree-based models in combination with conventional SMT models provides a promising approach that incorporates the power of [[ phrasal SMT ]] with the linguistic generality available in a << parser >> ."}
{"label": "FEATURE-OF", "text": "We describe an efficient decoder and show that using these tree-based models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the [[ linguistic generality ]] available in a << parser >> ."}
{"label": "FEATURE-OF", "text": "<< Video >> provides not only rich [[ visual cues ]] such as motion and appearance , but also much less explored long-range temporal interactions among objects ."}
{"label": "HYPONYM-OF", "text": "Video provides not only rich << visual cues >> such as [[ motion ]] and appearance , but also much less explored long-range temporal interactions among objects ."}
{"label": "CONJUNCTION", "text": "Video provides not only rich visual cues such as [[ motion ]] and << appearance >> , but also much less explored long-range temporal interactions among objects ."}
{"label": "HYPONYM-OF", "text": "Video provides not only rich << visual cues >> such as motion and [[ appearance ]] , but also much less explored long-range temporal interactions among objects ."}
{"label": "USED-FOR", "text": "We aim to capture such interactions and to construct a powerful [[ intermediate-level video representation ]] for subsequent << recognition >> ."}
{"label": "USED-FOR", "text": "First , we develop an efficient << spatio-temporal video segmentation algorithm >> , which naturally incorporates [[ long-range motion cues ]] from the past and future frames in the form of clusters of point tracks with coherent motion ."}
{"label": "USED-FOR", "text": "First , we develop an efficient spatio-temporal video segmentation algorithm , which naturally incorporates << long-range motion cues >> from the past and future frames in the form of [[ clusters of point tracks ]] with coherent motion ."}
{"label": "PART-OF", "text": "Second , we devise a new << track clustering cost function >> that includes [[ occlusion reasoning ]] , in the form of depth ordering constraints , as well as motion similarity along the tracks ."}
{"label": "FEATURE-OF", "text": "Second , we devise a new track clustering cost function that includes << occlusion reasoning >> , in the form of [[ depth ordering constraints ]] , as well as motion similarity along the tracks ."}
{"label": "PART-OF", "text": "Second , we devise a new << track clustering cost function >> that includes occlusion reasoning , in the form of depth ordering constraints , as well as [[ motion similarity ]] along the tracks ."}
{"label": "EVALUATE-FOR", "text": "We evaluate the proposed << approach >> on a challenging set of [[ video sequences of office scenes ]] from feature length movies ."}
{"label": "HYPONYM-OF", "text": "In this paper , we introduce [[ KAZE features ]] , a novel << multiscale 2D feature detection and description algorithm >> in nonlinear scale spaces ."}
{"label": "FEATURE-OF", "text": "In this paper , we introduce KAZE features , a novel << multiscale 2D feature detection and description algorithm >> in [[ nonlinear scale spaces ]] ."}
{"label": "FEATURE-OF", "text": "In contrast , we detect and describe << 2D features >> in a [[ nonlinear scale space ]] by means of nonlinear diffusion filtering ."}
{"label": "USED-FOR", "text": "In contrast , we detect and describe << 2D features >> in a nonlinear scale space by means of [[ nonlinear diffusion filtering ]] ."}
{"label": "USED-FOR", "text": "The << nonlinear scale space >> is built using efficient [[ Additive Operator Splitting -LRB- AOS -RRB- techniques ]] and variable con-ductance diffusion ."}
{"label": "CONJUNCTION", "text": "The nonlinear scale space is built using efficient [[ Additive Operator Splitting -LRB- AOS -RRB- techniques ]] and << variable con-ductance diffusion >> ."}
{"label": "USED-FOR", "text": "The << nonlinear scale space >> is built using efficient Additive Operator Splitting -LRB- AOS -RRB- techniques and [[ variable con-ductance diffusion ]] ."}
{"label": "COMPARE", "text": "Even though our [[ features ]] are somewhat more expensive to compute than << SURF >> due to the construction of the nonlinear scale space , but comparable to SIFT , our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods ."}
{"label": "COMPARE", "text": "Even though our [[ features ]] are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space , but comparable to << SIFT >> , our results reveal a step forward in performance both in detection and description against previous state-of-the-art methods ."}
{"label": "COMPARE", "text": "Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space , but comparable to SIFT , our [[ results ]] reveal a step forward in performance both in detection and description against previous << state-of-the-art methods >> ."}
{"label": "EVALUATE-FOR", "text": "Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space , but comparable to SIFT , our << results >> reveal a step forward in performance both in [[ detection ]] and description against previous state-of-the-art methods ."}
{"label": "CONJUNCTION", "text": "Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space , but comparable to SIFT , our results reveal a step forward in performance both in [[ detection ]] and << description >> against previous state-of-the-art methods ."}
{"label": "EVALUATE-FOR", "text": "Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space , but comparable to SIFT , our results reveal a step forward in performance both in [[ detection ]] and description against previous << state-of-the-art methods >> ."}
{"label": "EVALUATE-FOR", "text": "Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space , but comparable to SIFT , our << results >> reveal a step forward in performance both in detection and [[ description ]] against previous state-of-the-art methods ."}
{"label": "EVALUATE-FOR", "text": "Even though our features are somewhat more expensive to compute than SURF due to the construction of the nonlinear scale space , but comparable to SIFT , our results reveal a step forward in performance both in detection and [[ description ]] against previous << state-of-the-art methods >> ."}
{"label": "USED-FOR", "text": "[[ Creating summaries ]] on lengthy Semantic Web documents for quick << identification of the corresponding entity >> has been of great contemporary interest ."}
{"label": "USED-FOR", "text": "<< Creating summaries >> on [[ lengthy Semantic Web documents ]] for quick identification of the corresponding entity has been of great contemporary interest ."}
{"label": "FEATURE-OF", "text": "Specifically , we highlight the importance of << diversified -LRB- faceted -RRB- summaries >> by combining three dimensions : [[ diversity ]] , uniqueness , and popularity ."}
{"label": "CONJUNCTION", "text": "Specifically , we highlight the importance of diversified -LRB- faceted -RRB- summaries by combining three dimensions : [[ diversity ]] , << uniqueness >> , and popularity ."}
{"label": "FEATURE-OF", "text": "Specifically , we highlight the importance of << diversified -LRB- faceted -RRB- summaries >> by combining three dimensions : diversity , [[ uniqueness ]] , and popularity ."}
{"label": "CONJUNCTION", "text": "Specifically , we highlight the importance of diversified -LRB- faceted -RRB- summaries by combining three dimensions : diversity , [[ uniqueness ]] , and << popularity >> ."}
{"label": "FEATURE-OF", "text": "Specifically , we highlight the importance of << diversified -LRB- faceted -RRB- summaries >> by combining three dimensions : diversity , uniqueness , and [[ popularity ]] ."}
{"label": "USED-FOR", "text": "Our novel << diversity-aware entity summarization approach >> mimics [[ human conceptual clustering techniques ]] to group facts , and picks representative facts from each group to form concise -LRB- i.e. , short -RRB- and comprehensive -LRB- i.e. , improved coverage through diversity -RRB- summaries ."}
{"label": "USED-FOR", "text": "We evaluate our [[ approach ]] against the state-of-the-art techniques and show that our work improves both the quality and the efficiency of << entity summarization >> ."}
{"label": "COMPARE", "text": "We evaluate our << approach >> against the [[ state-of-the-art techniques ]] and show that our work improves both the quality and the efficiency of entity summarization ."}
{"label": "USED-FOR", "text": "We evaluate our approach against the [[ state-of-the-art techniques ]] and show that our work improves both the quality and the efficiency of << entity summarization >> ."}
{"label": "EVALUATE-FOR", "text": "We evaluate our approach against the state-of-the-art techniques and show that our work improves both the [[ quality ]] and the efficiency of << entity summarization >> ."}
{"label": "EVALUATE-FOR", "text": "We evaluate our approach against the state-of-the-art techniques and show that our work improves both the quality and the [[ efficiency ]] of << entity summarization >> ."}
{"label": "USED-FOR", "text": "We present a [[ framework ]] for the << fast computation of lexical affinity models >> ."}
{"label": "PART-OF", "text": "The << framework >> is composed of a novel [[ algorithm ]] to efficiently compute the co-occurrence distribution between pairs of terms , an independence model , and a parametric affinity model ."}
{"label": "USED-FOR", "text": "The framework is composed of a novel [[ algorithm ]] to efficiently compute the << co-occurrence distribution >> between pairs of terms , an independence model , and a parametric affinity model ."}
{"label": "CONJUNCTION", "text": "The framework is composed of a novel [[ algorithm ]] to efficiently compute the co-occurrence distribution between pairs of terms , an << independence model >> , and a parametric affinity model ."}
{"label": "PART-OF", "text": "The << framework >> is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms , an [[ independence model ]] , and a parametric affinity model ."}
{"label": "CONJUNCTION", "text": "The framework is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms , an [[ independence model ]] , and a << parametric affinity model >> ."}
{"label": "PART-OF", "text": "The << framework >> is composed of a novel algorithm to efficiently compute the co-occurrence distribution between pairs of terms , an independence model , and a [[ parametric affinity model ]] ."}
{"label": "USED-FOR", "text": "In comparison with previous models , which either use arbitrary windows to compute similarity between words or use [[ lexical affinity ]] to create << sequential models >> , in this paper we focus on models intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus ."}
{"label": "COMPARE", "text": "In comparison with previous << models >> , which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models , in this paper we focus on [[ models ]] intended to capture the co-occurrence patterns of any pair of words or phrases at any distance in the corpus ."}
{"label": "USED-FOR", "text": "In comparison with previous models , which either use arbitrary windows to compute similarity between words or use lexical affinity to create sequential models , in this paper we focus on [[ models ]] intended to capture the << co-occurrence patterns >> of any pair of words or phrases at any distance in the corpus ."}
{"label": "USED-FOR", "text": "We apply [[ it ]] in combination with a terabyte corpus to answer << natural language tests >> , achieving encouraging results ."}
{"label": "EVALUATE-FOR", "text": "We apply << it >> in combination with a [[ terabyte corpus ]] to answer natural language tests , achieving encouraging results ."}
{"label": "USED-FOR", "text": "This paper introduces a [[ system ]] for << categorizing unknown words >> ."}
{"label": "USED-FOR", "text": "The << system >> is based on a [[ multi-component architecture ]] where each component is responsible for identifying one class of unknown words ."}
{"label": "PART-OF", "text": "The system is based on a << multi-component architecture >> where each [[ component ]] is responsible for identifying one class of unknown words ."}
{"label": "USED-FOR", "text": "The system is based on a multi-component architecture where each [[ component ]] is responsible for identifying one class of << unknown words >> ."}
{"label": "USED-FOR", "text": "The focus of this paper is the [[ components ]] that identify << names >> and spelling errors ."}
{"label": "USED-FOR", "text": "The focus of this paper is the [[ components ]] that identify names and << spelling errors >> ."}
{"label": "CONJUNCTION", "text": "The focus of this paper is the components that identify [[ names ]] and << spelling errors >> ."}
{"label": "USED-FOR", "text": "Each << component >> uses a [[ decision tree architecture ]] to combine multiple types of evidence about the unknown word ."}
{"label": "EVALUATE-FOR", "text": "The << system >> is evaluated using data from [[ live closed captions ]] - a genre replete with a wide variety of unknown words ."}
{"label": "HYPONYM-OF", "text": "At MIT Lincoln Laboratory , we have been developing a << Korean-to-English machine translation system >> [[ CCLINC -LRB- Common Coalition Language System at Lincoln Laboratory -RRB- ]] ."}
{"label": "PART-OF", "text": "The << CCLINC Korean-to-English translation system >> consists of two [[ core modules ]] , language understanding and generation modules mediated by a language neutral meaning representation called a semantic frame ."}
{"label": "USED-FOR", "text": "The CCLINC Korean-to-English translation system consists of two core modules , << language understanding and generation modules >> mediated by a [[ language neutral meaning representation ]] called a semantic frame ."}
{"label": "HYPONYM-OF", "text": "The CCLINC Korean-to-English translation system consists of two core modules , language understanding and generation modules mediated by a << language neutral meaning representation >> called a [[ semantic frame ]] ."}
{"label": "HYPONYM-OF", "text": "The key features of the system include : -LRB- i -RRB- Robust efficient parsing of [[ Korean ]] -LRB- a << verb final language >> with overt case markers , relatively free word order , and frequent omissions of arguments -RRB- ."}
{"label": "FEATURE-OF", "text": "The key features of the system include : -LRB- i -RRB- Robust efficient parsing of Korean -LRB- a << verb final language >> with [[ overt case markers ]] , relatively free word order , and frequent omissions of arguments -RRB- ."}
{"label": "USED-FOR", "text": "-LRB- ii -RRB- High quality << translation >> via [[ word sense disambiguation ]] and accurate word order generation of the target language ."}
{"label": "CONJUNCTION", "text": "-LRB- ii -RRB- High quality translation via [[ word sense disambiguation ]] and accurate << word order generation >> of the target language ."}
{"label": "USED-FOR", "text": "-LRB- ii -RRB- High quality << translation >> via word sense disambiguation and accurate [[ word order generation ]] of the target language ."}
{"label": "USED-FOR", "text": "Having been trained on [[ Korean newspaper articles ]] on missiles and chemical biological warfare , the << system >> produces the translation output sufficient for content understanding of the original document ."}
{"label": "FEATURE-OF", "text": "Having been trained on << Korean newspaper articles >> on [[ missiles and chemical biological warfare ]] , the system produces the translation output sufficient for content understanding of the original document ."}
{"label": "USED-FOR", "text": "The [[ JAVELIN system ]] integrates a flexible , planning-based architecture with a variety of language processing modules to provide an << open-domain question answering capability >> on free text ."}
{"label": "PART-OF", "text": "The << JAVELIN system >> integrates a flexible , [[ planning-based architecture ]] with a variety of language processing modules to provide an open-domain question answering capability on free text ."}
{"label": "PART-OF", "text": "The << JAVELIN system >> integrates a flexible , planning-based architecture with a variety of [[ language processing modules ]] to provide an open-domain question answering capability on free text ."}
{"label": "CONJUNCTION", "text": "The JAVELIN system integrates a flexible , << planning-based architecture >> with a variety of [[ language processing modules ]] to provide an open-domain question answering capability on free text ."}
{"label": "USED-FOR", "text": "We present the first application of the [[ head-driven statistical parsing model ]] of Collins -LRB- 1999 -RRB- as a << simultaneous language model >> and parser for large-vocabulary speech recognition ."}
{"label": "USED-FOR", "text": "We present the first application of the [[ head-driven statistical parsing model ]] of Collins -LRB- 1999 -RRB- as a simultaneous language model and << parser >> for large-vocabulary speech recognition ."}
{"label": "CONJUNCTION", "text": "We present the first application of the head-driven statistical parsing model of Collins -LRB- 1999 -RRB- as a [[ simultaneous language model ]] and << parser >> for large-vocabulary speech recognition ."}
{"label": "USED-FOR", "text": "We present the first application of the head-driven statistical parsing model of Collins -LRB- 1999 -RRB- as a [[ simultaneous language model ]] and parser for << large-vocabulary speech recognition >> ."}
{"label": "USED-FOR", "text": "We present the first application of the head-driven statistical parsing model of Collins -LRB- 1999 -RRB- as a simultaneous language model and [[ parser ]] for << large-vocabulary speech recognition >> ."}
{"label": "USED-FOR", "text": "The [[ model ]] is adapted to an << online left to right chart-parser >> for word lattices , integrating acoustic , n-gram , and parser probabilities ."}
{"label": "USED-FOR", "text": "The model is adapted to an [[ online left to right chart-parser ]] for << word lattices >> , integrating acoustic , n-gram , and parser probabilities ."}
{"label": "PART-OF", "text": "The model is adapted to an << online left to right chart-parser >> for word lattices , integrating [[ acoustic , n-gram , and parser probabilities ]] ."}
{"label": "USED-FOR", "text": "The << parser >> uses [[ structural and lexical dependencies ]] not considered by n-gram models , conditioning recognition on more linguistically-grounded relationships ."}
{"label": "CONJUNCTION", "text": "Experiments on the [[ Wall Street Journal treebank ]] and << lattice corpora >> show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding ."}
{"label": "EVALUATE-FOR", "text": "Experiments on the [[ Wall Street Journal treebank ]] and lattice corpora show word error rates competitive with the standard << n-gram language model >> while extracting additional structural information useful for speech understanding ."}
{"label": "EVALUATE-FOR", "text": "Experiments on the Wall Street Journal treebank and [[ lattice corpora ]] show word error rates competitive with the standard << n-gram language model >> while extracting additional structural information useful for speech understanding ."}
{"label": "EVALUATE-FOR", "text": "Experiments on the Wall Street Journal treebank and lattice corpora show [[ word error rates ]] competitive with the standard << n-gram language model >> while extracting additional structural information useful for speech understanding ."}
{"label": "USED-FOR", "text": "Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional [[ structural information ]] useful for << speech understanding >> ."}
{"label": "PART-OF", "text": "[[ Image composition -LRB- or mosaicing -RRB- ]] has attracted a growing attention in recent years as one of the main elements in << video analysis and representation >> ."}
{"label": "CONJUNCTION", "text": "In this paper we deal with the problem of [[ global alignment ]] and << super-resolution >> ."}
{"label": "EVALUATE-FOR", "text": "We also propose to evaluate the quality of the resulting << mosaic >> by measuring the [[ amount of blurring ]] ."}
{"label": "USED-FOR", "text": "<< Global registration >> is achieved by combining a [[ graph-based technique ]] -- that exploits the topological structure of the sequence induced by the spatial overlap -- with a bundle adjustment which uses only the homographies computed in the previous steps ."}
{"label": "USED-FOR", "text": "Global registration is achieved by combining a [[ graph-based technique ]] -- that exploits the << topological structure >> of the sequence induced by the spatial overlap -- with a bundle adjustment which uses only the homographies computed in the previous steps ."}
{"label": "CONJUNCTION", "text": "Global registration is achieved by combining a [[ graph-based technique ]] -- that exploits the topological structure of the sequence induced by the spatial overlap -- with a << bundle adjustment >> which uses only the homographies computed in the previous steps ."}
{"label": "USED-FOR", "text": "<< Global registration >> is achieved by combining a graph-based technique -- that exploits the topological structure of the sequence induced by the spatial overlap -- with a [[ bundle adjustment ]] which uses only the homographies computed in the previous steps ."}
{"label": "USED-FOR", "text": "Global registration is achieved by combining a graph-based technique -- that exploits the topological structure of the sequence induced by the spatial overlap -- with a << bundle adjustment >> which uses only the [[ homographies ]] computed in the previous steps ."}
{"label": "COMPARE", "text": "Experimental comparison with other << techniques >> shows the effectiveness of our [[ approach ]] ."}
{"label": "USED-FOR", "text": "The main of this project is << computer-assisted acquisition and morpho-syntactic description of verb-noun collocations >> in [[ Polish ]] ."}
{"label": "HYPONYM-OF", "text": "We present methodology and resources obtained in three main project << phases >> which are : [[ dictionary-based acquisition of collocation lexicon ]] , feasibility study for corpus-based lexicon enlargement phase , corpus-based lexicon enlargement and collocation description ."}
{"label": "CONJUNCTION", "text": "We present methodology and resources obtained in three main project phases which are : [[ dictionary-based acquisition of collocation lexicon ]] , << feasibility study >> for corpus-based lexicon enlargement phase , corpus-based lexicon enlargement and collocation description ."}
{"label": "HYPONYM-OF", "text": "We present methodology and resources obtained in three main project << phases >> which are : dictionary-based acquisition of collocation lexicon , [[ feasibility study ]] for corpus-based lexicon enlargement phase , corpus-based lexicon enlargement and collocation description ."}
{"label": "USED-FOR", "text": "We present methodology and resources obtained in three main project phases which are : dictionary-based acquisition of collocation lexicon , [[ feasibility study ]] for << corpus-based lexicon enlargement phase >> , corpus-based lexicon enlargement and collocation description ."}
{"label": "HYPONYM-OF", "text": "We present methodology and resources obtained in three main project << phases >> which are : dictionary-based acquisition of collocation lexicon , feasibility study for corpus-based lexicon enlargement phase , [[ corpus-based lexicon enlargement and collocation description ]] ."}
{"label": "CONJUNCTION", "text": "We present methodology and resources obtained in three main project phases which are : dictionary-based acquisition of collocation lexicon , << feasibility study >> for corpus-based lexicon enlargement phase , [[ corpus-based lexicon enlargement and collocation description ]] ."}
{"label": "USED-FOR", "text": "The presented here [[ corpus-based approach ]] permitted us to triple the size the << verb-noun collocation dictionary >> for Polish ."}
{"label": "FEATURE-OF", "text": "The presented here corpus-based approach permitted us to triple the size the << verb-noun collocation dictionary >> for [[ Polish ]] ."}
{"label": "USED-FOR", "text": "Along with the increasing requirements , the [[ hash-tag recommendation task ]] for << microblogs >> has been receiving considerable attention in recent years ."}
{"label": "USED-FOR", "text": "Motivated by the successful use of [[ convolutional neural networks -LRB- CNNs -RRB- ]] for many << natural language processing tasks >> , in this paper , we adopt CNNs to perform the hashtag recommendation problem ."}
{"label": "USED-FOR", "text": "To incorporate the << trigger words >> whose effectiveness have been experimentally evaluated in several previous works , we propose a novel [[ architecture ]] with an attention mechanism ."}
{"label": "FEATURE-OF", "text": "To incorporate the trigger words whose effectiveness have been experimentally evaluated in several previous works , we propose a novel << architecture >> with an [[ attention mechanism ]] ."}
{"label": "EVALUATE-FOR", "text": "The results of experiments on the [[ data ]] collected from a real world microblogging service demonstrated that the proposed << model >> outperforms state-of-the-art methods ."}
{"label": "COMPARE", "text": "The results of experiments on the data collected from a real world microblogging service demonstrated that the proposed [[ model ]] outperforms << state-of-the-art methods >> ."}
{"label": "COMPARE", "text": "By incorporating trigger words into the consideration , the relative improvement of the proposed [[ method ]] over the << state-of-the-art method >> is around 9.4 % in the F1-score ."}
{"label": "EVALUATE-FOR", "text": "By incorporating trigger words into the consideration , the relative improvement of the proposed method over the << state-of-the-art method >> is around 9.4 % in the [[ F1-score ]] ."}
{"label": "USED-FOR", "text": "In this paper , we improve an << unsupervised learning method >> using the [[ Expectation-Maximization -LRB- EM -RRB- algorithm ]] proposed by Nigam et al. for text classification problems in order to apply it to word sense disambiguation -LRB- WSD -RRB- problems ."}
{"label": "USED-FOR", "text": "In this paper , we improve an unsupervised learning method using the [[ Expectation-Maximization -LRB- EM -RRB- algorithm ]] proposed by Nigam et al. for << text classification problems >> in order to apply it to word sense disambiguation -LRB- WSD -RRB- problems ."}
{"label": "USED-FOR", "text": "In this paper , we improve an unsupervised learning method using the Expectation-Maximization -LRB- EM -RRB- algorithm proposed by Nigam et al. for text classification problems in order to apply [[ it ]] to << word sense disambiguation -LRB- WSD -RRB- problems >> ."}
{"label": "FEATURE-OF", "text": "In experiments , we solved 50 noun WSD problems in the [[ Japanese Dictionary Task ]] in << SENSEVAL2 >> ."}
{"label": "USED-FOR", "text": "Furthermore , our [[ methods ]] were confirmed to be effective also for << verb WSD problems >> ."}
{"label": "USED-FOR", "text": "[[ Dividing sentences in chunks of words ]] is a useful preprocessing step for << parsing >> , information extraction and information retrieval ."}
{"label": "USED-FOR", "text": "[[ Dividing sentences in chunks of words ]] is a useful preprocessing step for parsing , << information extraction >> and information retrieval ."}
{"label": "USED-FOR", "text": "[[ Dividing sentences in chunks of words ]] is a useful preprocessing step for parsing , information extraction and << information retrieval >> ."}
{"label": "CONJUNCTION", "text": "Dividing sentences in chunks of words is a useful preprocessing step for [[ parsing ]] , << information extraction >> and information retrieval ."}
{"label": "CONJUNCTION", "text": "Dividing sentences in chunks of words is a useful preprocessing step for parsing , [[ information extraction ]] and << information retrieval >> ."}
{"label": "USED-FOR", "text": "-LRB- Ramshaw and Marcus , 1995 -RRB- have introduced a `` convenient '' [[ data representation ]] for << chunking >> by converting it to a tagging task ."}
{"label": "USED-FOR", "text": "In this paper we will examine seven different [[ data representations ]] for the problem of << recognizing noun phrase chunks >> ."}
{"label": "USED-FOR", "text": "However , equipped with the most suitable [[ data representation ]] , our << memory-based learning chunker >> was able to improve the best published chunking results for a standard data set ."}
{"label": "EVALUATE-FOR", "text": "However , equipped with the most suitable data representation , our << memory-based learning chunker >> was able to improve the best published chunking results for a standard [[ data set ]] ."}
{"label": "USED-FOR", "text": "We focus on << FAQ-like questions and answers >> , and build our [[ system ]] around a noisy-channel architecture which exploits both a language model for answers and a transformation model for answer/question terms , trained on a corpus of 1 million question/answer pairs collected from the Web ."}
{"label": "USED-FOR", "text": "We focus on FAQ-like questions and answers , and build our << system >> around a [[ noisy-channel architecture ]] which exploits both a language model for answers and a transformation model for answer/question terms , trained on a corpus of 1 million question/answer pairs collected from the Web ."}
{"label": "USED-FOR", "text": "We focus on FAQ-like questions and answers , and build our system around a [[ noisy-channel architecture ]] which exploits both a << language model >> for answers and a transformation model for answer/question terms , trained on a corpus of 1 million question/answer pairs collected from the Web ."}
{"label": "USED-FOR", "text": "We focus on FAQ-like questions and answers , and build our system around a [[ noisy-channel architecture ]] which exploits both a language model for answers and a << transformation model >> for answer/question terms , trained on a corpus of 1 million question/answer pairs collected from the Web ."}
{"label": "EVALUATE-FOR", "text": "In this paper we evaluate four objective [[ measures of speech ]] with regards to << intelligibility prediction >> of synthesized speech in diverse noisy situations ."}
{"label": "USED-FOR", "text": "In this paper we evaluate four objective measures of speech with regards to << intelligibility prediction >> of [[ synthesized speech ]] in diverse noisy situations ."}
{"label": "FEATURE-OF", "text": "In this paper we evaluate four objective measures of speech with regards to intelligibility prediction of << synthesized speech >> in [[ diverse noisy situations ]] ."}
{"label": "CONJUNCTION", "text": "We evaluated three [[ intel-ligibility measures ]] , the Dau measure , the glimpse proportion and the Speech Intelligibility Index -LRB- SII -RRB- and a << quality measure >> , the Perceptual Evaluation of Speech Quality -LRB- PESQ -RRB- ."}
{"label": "HYPONYM-OF", "text": "We evaluated three << intel-ligibility measures >> , the [[ Dau measure ]] , the glimpse proportion and the Speech Intelligibility Index -LRB- SII -RRB- and a quality measure , the Perceptual Evaluation of Speech Quality -LRB- PESQ -RRB- ."}
{"label": "CONJUNCTION", "text": "We evaluated three intel-ligibility measures , the [[ Dau measure ]] , the << glimpse proportion >> and the Speech Intelligibility Index -LRB- SII -RRB- and a quality measure , the Perceptual Evaluation of Speech Quality -LRB- PESQ -RRB- ."}
{"label": "HYPONYM-OF", "text": "We evaluated three << intel-ligibility measures >> , the Dau measure , the [[ glimpse proportion ]] and the Speech Intelligibility Index -LRB- SII -RRB- and a quality measure , the Perceptual Evaluation of Speech Quality -LRB- PESQ -RRB- ."}
{"label": "CONJUNCTION", "text": "We evaluated three intel-ligibility measures , the Dau measure , the [[ glimpse proportion ]] and the << Speech Intelligibility Index -LRB- SII -RRB- >> and a quality measure , the Perceptual Evaluation of Speech Quality -LRB- PESQ -RRB- ."}
{"label": "HYPONYM-OF", "text": "We evaluated three << intel-ligibility measures >> , the Dau measure , the glimpse proportion and the [[ Speech Intelligibility Index -LRB- SII -RRB- ]] and a quality measure , the Perceptual Evaluation of Speech Quality -LRB- PESQ -RRB- ."}
{"label": "HYPONYM-OF", "text": "We evaluated three intel-ligibility measures , the Dau measure , the glimpse proportion and the Speech Intelligibility Index -LRB- SII -RRB- and a << quality measure >> , the [[ Perceptual Evaluation of Speech Quality -LRB- PESQ -RRB- ]] ."}
{"label": "USED-FOR", "text": "For the << generation of synthesized speech >> we used a state of the art [[ HMM-based speech synthesis system ]] ."}
{"label": "PART-OF", "text": "The << noisy conditions >> comprised four [[ additive noises ]] ."}
{"label": "COMPARE", "text": "The [[ measures ]] were compared with << subjective intelligibility scores >> obtained in listening tests ."}
{"label": "CONJUNCTION", "text": "The results show the [[ Dau ]] and the << glimpse measures >> to be the best predictors of intelligibility , with correlations of around 0.83 to subjective scores ."}
{"label": "HYPONYM-OF", "text": "The results show the [[ Dau ]] and the glimpse measures to be the best << predictors of intelligibility >> , with correlations of around 0.83 to subjective scores ."}
{"label": "COMPARE", "text": "The results show the [[ Dau ]] and the glimpse measures to be the best predictors of intelligibility , with correlations of around 0.83 to << subjective scores >> ."}
{"label": "HYPONYM-OF", "text": "The results show the Dau and the [[ glimpse measures ]] to be the best << predictors of intelligibility >> , with correlations of around 0.83 to subjective scores ."}
{"label": "COMPARE", "text": "The results show the Dau and the [[ glimpse measures ]] to be the best predictors of intelligibility , with correlations of around 0.83 to << subjective scores >> ."}
{"label": "EVALUATE-FOR", "text": "The results show the << Dau >> and the glimpse measures to be the best predictors of intelligibility , with [[ correlations ]] of around 0.83 to subjective scores ."}
{"label": "EVALUATE-FOR", "text": "The results show the Dau and the << glimpse measures >> to be the best predictors of intelligibility , with [[ correlations ]] of around 0.83 to subjective scores ."}
{"label": "EVALUATE-FOR", "text": "All [[ measures ]] gave less accurate << predictions of intelligibility >> for synthetic speech than have previously been found for natural speech ; in particular the SII measure ."}
{"label": "USED-FOR", "text": "All measures gave less accurate << predictions of intelligibility >> for [[ synthetic speech ]] than have previously been found for natural speech ; in particular the SII measure ."}
{"label": "COMPARE", "text": "All measures gave less accurate predictions of intelligibility for [[ synthetic speech ]] than have previously been found for << natural speech >> ; in particular the SII measure ."}
{"label": "HYPONYM-OF", "text": "All << measures >> gave less accurate predictions of intelligibility for synthetic speech than have previously been found for natural speech ; in particular the [[ SII measure ]] ."}
{"label": "USED-FOR", "text": "In additional experiments , we processed the << synthesized speech >> by an [[ ideal binary mask ]] before adding noise ."}
{"label": "USED-FOR", "text": "The [[ Glimpse measure ]] gave the most accurate << intelligibility predictions >> in this situation ."}
{"label": "USED-FOR", "text": "A [[ '' graphics for vision '' approach ]] is proposed to address the problem of << reconstruction >> from a large and imperfect data set : reconstruction on demand by tensor voting , or ROD-TV ."}
{"label": "USED-FOR", "text": "A '' graphics for vision '' approach is proposed to address the problem of << reconstruction >> from a [[ large and imperfect data set ]] : reconstruction on demand by tensor voting , or ROD-TV ."}
{"label": "USED-FOR", "text": "A '' graphics for vision '' approach is proposed to address the problem of reconstruction from a large and imperfect data set : << reconstruction >> on demand by [[ tensor voting ]] , or ROD-TV ."}
{"label": "CONJUNCTION", "text": "A '' graphics for vision '' approach is proposed to address the problem of reconstruction from a large and imperfect data set : reconstruction on demand by [[ tensor voting ]] , or << ROD-TV >> ."}
{"label": "USED-FOR", "text": "A '' graphics for vision '' approach is proposed to address the problem of reconstruction from a large and imperfect data set : << reconstruction >> on demand by tensor voting , or [[ ROD-TV ]] ."}
{"label": "EVALUATE-FOR", "text": "<< ROD-TV >> simultaneously delivers good [[ efficiency ]] and robust-ness , by adapting to a continuum of primitive connectivity , view dependence , and levels of detail -LRB- LOD -RRB- ."}
{"label": "EVALUATE-FOR", "text": "<< ROD-TV >> simultaneously delivers good efficiency and [[ robust-ness ]] , by adapting to a continuum of primitive connectivity , view dependence , and levels of detail -LRB- LOD -RRB- ."}
{"label": "CONJUNCTION", "text": "ROD-TV simultaneously delivers good << efficiency >> and [[ robust-ness ]] , by adapting to a continuum of primitive connectivity , view dependence , and levels of detail -LRB- LOD -RRB- ."}
{"label": "CONJUNCTION", "text": "ROD-TV simultaneously delivers good efficiency and robust-ness , by adapting to a continuum of << primitive connectivity >> , [[ view dependence ]] , and levels of detail -LRB- LOD -RRB- ."}
{"label": "CONJUNCTION", "text": "ROD-TV simultaneously delivers good efficiency and robust-ness , by adapting to a continuum of primitive connectivity , << view dependence >> , and [[ levels of detail -LRB- LOD -RRB- ]] ."}
{"label": "USED-FOR", "text": "[[ Locally inferred surface elements ]] are robust to noise and better capture << local shapes >> ."}
{"label": "USED-FOR", "text": "By inferring [[ per-vertex normals ]] at sub-voxel precision on the fly , we can achieve << interpolative shading >> ."}
{"label": "FEATURE-OF", "text": "By inferring << per-vertex normals >> at [[ sub-voxel precision ]] on the fly , we can achieve interpolative shading ."}
{"label": "USED-FOR", "text": "By relaxing the [[ mesh connectivity requirement ]] , we extend ROD-TV and propose a simple but effective << multiscale feature extraction algorithm >> ."}
{"label": "USED-FOR", "text": "By relaxing the mesh connectivity requirement , we extend [[ ROD-TV ]] and propose a simple but effective << multiscale feature extraction algorithm >> ."}
{"label": "PART-OF", "text": "<< ROD-TV >> consists of a [[ hierarchical data structure ]] that encodes different levels of detail ."}
{"label": "HYPONYM-OF", "text": "The << local reconstruction algorithm >> is [[ tensor voting ]] ."}
{"label": "USED-FOR", "text": "<< It >> is applied on demand to the visible subset of data at a desired level of detail , by [[ traversing the data hierarchy ]] and collecting tensorial support in a neighborhood ."}
{"label": "CONJUNCTION", "text": "It is applied on demand to the visible subset of data at a desired level of detail , by [[ traversing the data hierarchy ]] and << collecting tensorial support >> in a neighborhood ."}
{"label": "USED-FOR", "text": "<< It >> is applied on demand to the visible subset of data at a desired level of detail , by traversing the data hierarchy and [[ collecting tensorial support ]] in a neighborhood ."}
{"label": "CONJUNCTION", "text": "Both [[ rhetorical structure ]] and << punctuation >> have been helpful in discourse processing ."}
{"label": "USED-FOR", "text": "Both [[ rhetorical structure ]] and punctuation have been helpful in << discourse processing >> ."}
{"label": "USED-FOR", "text": "Both rhetorical structure and [[ punctuation ]] have been helpful in << discourse processing >> ."}
{"label": "PART-OF", "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 [[ Chinese punctuation marks ]] in << news commentary texts >> : Colon , Dash , Ellipsis , Exclamation Mark , Question Mark , and Semicolon ."}
{"label": "HYPONYM-OF", "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 << Chinese punctuation marks >> in news commentary texts : [[ Colon ]] , Dash , Ellipsis , Exclamation Mark , Question Mark , and Semicolon ."}
{"label": "CONJUNCTION", "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts : [[ Colon ]] , << Dash >> , Ellipsis , Exclamation Mark , Question Mark , and Semicolon ."}
{"label": "HYPONYM-OF", "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 << Chinese punctuation marks >> in news commentary texts : Colon , [[ Dash ]] , Ellipsis , Exclamation Mark , Question Mark , and Semicolon ."}
{"label": "CONJUNCTION", "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts : Colon , [[ Dash ]] , << Ellipsis >> , Exclamation Mark , Question Mark , and Semicolon ."}
{"label": "HYPONYM-OF", "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 << Chinese punctuation marks >> in news commentary texts : Colon , Dash , [[ Ellipsis ]] , Exclamation Mark , Question Mark , and Semicolon ."}
{"label": "CONJUNCTION", "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts : Colon , Dash , [[ Ellipsis ]] , << Exclamation Mark >> , Question Mark , and Semicolon ."}
{"label": "HYPONYM-OF", "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 << Chinese punctuation marks >> in news commentary texts : Colon , Dash , Ellipsis , [[ Exclamation Mark ]] , Question Mark , and Semicolon ."}
{"label": "CONJUNCTION", "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts : Colon , Dash , Ellipsis , [[ Exclamation Mark ]] , << Question Mark >> , and Semicolon ."}
{"label": "HYPONYM-OF", "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 << Chinese punctuation marks >> in news commentary texts : Colon , Dash , Ellipsis , Exclamation Mark , [[ Question Mark ]] , and Semicolon ."}
{"label": "CONJUNCTION", "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 Chinese punctuation marks in news commentary texts : Colon , Dash , Ellipsis , Exclamation Mark , [[ Question Mark ]] , and << Semicolon >> ."}
{"label": "HYPONYM-OF", "text": "Based on a corpus annotation project , this paper reports the discursive usage of 6 << Chinese punctuation marks >> in news commentary texts : Colon , Dash , Ellipsis , Exclamation Mark , Question Mark , and [[ Semicolon ]] ."}
{"label": "FEATURE-OF", "text": "The [[ rhetorical patterns ]] of these << marks >> are compared against patterns around cue phrases in general ."}
{"label": "COMPARE", "text": "The [[ rhetorical patterns ]] of these marks are compared against << patterns around cue phrases >> in general ."}
{"label": "COMPARE", "text": "Results show that these [[ Chinese punctuation marks ]] , though fewer in number than << cue phrases >> , are easy to identify , have strong correlation with certain relations , and can be used as distinctive indicators of nuclearity in Chinese texts ."}
{"label": "USED-FOR", "text": "Results show that these [[ Chinese punctuation marks ]] , though fewer in number than cue phrases , are easy to identify , have strong correlation with certain relations , and can be used as distinctive << indicators of nuclearity >> in Chinese texts ."}
{"label": "FEATURE-OF", "text": "Results show that these Chinese punctuation marks , though fewer in number than cue phrases , are easy to identify , have strong correlation with certain relations , and can be used as distinctive << indicators of nuclearity >> in [[ Chinese texts ]] ."}
{"label": "USED-FOR", "text": "The << features >> based on [[ Markov random field -LRB- MRF -RRB- models ]] are usually sensitive to the rotation of image textures ."}
{"label": "USED-FOR", "text": "This paper develops an [[ anisotropic circular Gaussian MRF -LRB- ACGMRF -RRB- model ]] for << modelling rotated image textures >> and retrieving rotation-invariant texture features ."}
{"label": "USED-FOR", "text": "This paper develops an [[ anisotropic circular Gaussian MRF -LRB- ACGMRF -RRB- model ]] for modelling rotated image textures and << retrieving rotation-invariant texture features >> ."}
{"label": "CONJUNCTION", "text": "This paper develops an anisotropic circular Gaussian MRF -LRB- ACGMRF -RRB- model for [[ modelling rotated image textures ]] and << retrieving rotation-invariant texture features >> ."}
{"label": "FEATURE-OF", "text": "To overcome the [[ singularity problem ]] of the << least squares estimate -LRB- LSE -RRB- method >> , an approximate least squares estimate -LRB- ALSE -RRB- method is proposed to estimate the parameters of the ACGMRF model ."}
{"label": "USED-FOR", "text": "To overcome the singularity problem of the least squares estimate -LRB- LSE -RRB- method , an [[ approximate least squares estimate -LRB- ALSE -RRB- method ]] is proposed to estimate the << parameters of the ACGMRF model >> ."}
{"label": "USED-FOR", "text": "The << rotation-invariant features >> can be obtained from the [[ parameters of the ACGMRF model ]] by the one-dimensional -LRB- 1-D -RRB- discrete Fourier transform -LRB- DFT -RRB- ."}
{"label": "USED-FOR", "text": "The << rotation-invariant features >> can be obtained from the parameters of the ACGMRF model by the [[ one-dimensional -LRB- 1-D -RRB- discrete Fourier transform -LRB- DFT -RRB- ]] ."}
{"label": "USED-FOR", "text": "Significantly improved accuracy can be achieved by applying the [[ rotation-invariant features ]] to classify << SAR -LRB- synthetic aperture radar >> -RRB- sea ice and Brodatz imagery ."}
{"label": "USED-FOR", "text": "Despite much recent progress on accurate << semantic role labeling >> , previous work has largely used [[ independent classifiers ]] , possibly combined with separate label sequence models via Viterbi decoding ."}
{"label": "CONJUNCTION", "text": "Despite much recent progress on accurate semantic role labeling , previous work has largely used [[ independent classifiers ]] , possibly combined with separate << label sequence models >> via Viterbi decoding ."}
{"label": "USED-FOR", "text": "Despite much recent progress on accurate semantic role labeling , previous work has largely used independent classifiers , possibly combined with separate << label sequence models >> via [[ Viterbi decoding ]] ."}
{"label": "PART-OF", "text": "We show how to build a joint model of argument frames , incorporating novel [[ features ]] that model these interactions into << discriminative log-linear models >> ."}
{"label": "EVALUATE-FOR", "text": "This << system >> achieves an [[ error reduction ]] of 22 % on all arguments and 32 % on core arguments over a state-of-the art independent classifier for gold-standard parse trees on PropBank ."}
{"label": "EVALUATE-FOR", "text": "This system achieves an [[ error reduction ]] of 22 % on all arguments and 32 % on core arguments over a state-of-the art << independent classifier >> for gold-standard parse trees on PropBank ."}
{"label": "COMPARE", "text": "This << system >> achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state-of-the art [[ independent classifier ]] for gold-standard parse trees on PropBank ."}
{"label": "EVALUATE-FOR", "text": "This << system >> achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state-of-the art independent classifier for [[ gold-standard parse trees ]] on PropBank ."}
{"label": "EVALUATE-FOR", "text": "This system achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state-of-the art << independent classifier >> for [[ gold-standard parse trees ]] on PropBank ."}
{"label": "PART-OF", "text": "This system achieves an error reduction of 22 % on all arguments and 32 % on core arguments over a state-of-the art independent classifier for [[ gold-standard parse trees ]] on << PropBank >> ."}
{"label": "USED-FOR", "text": "In order to deal with << ambiguity >> , the [[ MORphological PArser MORPA ]] is provided with a probabilistic context-free grammar -LRB- PCFG -RRB- , i.e. it combines a `` conventional '' context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse ."}
{"label": "USED-FOR", "text": "In order to deal with ambiguity , the << MORphological PArser MORPA >> is provided with a [[ probabilistic context-free grammar -LRB- PCFG -RRB- ]] , i.e. it combines a `` conventional '' context-free morphological grammar to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse ."}
{"label": "USED-FOR", "text": "In order to deal with ambiguity , the MORphological PArser MORPA is provided with a probabilistic context-free grammar -LRB- PCFG -RRB- , i.e. << it >> combines a [[ `` conventional '' context-free morphological grammar ]] to filter out ungrammatical segmentations with a probability-based scoring function which determines the likelihood of each successful parse ."}
{"label": "USED-FOR", "text": "In order to deal with ambiguity , the MORphological PArser MORPA is provided with a probabilistic context-free grammar -LRB- PCFG -RRB- , i.e. it combines a [[ `` conventional '' context-free morphological grammar ]] to filter out << ungrammatical segmentations >> with a probability-based scoring function which determines the likelihood of each successful parse ."}
{"label": "USED-FOR", "text": "In order to deal with ambiguity , the MORphological PArser MORPA is provided with a probabilistic context-free grammar -LRB- PCFG -RRB- , i.e. << it >> combines a `` conventional '' context-free morphological grammar to filter out ungrammatical segmentations with a [[ probability-based scoring function ]] which determines the likelihood of each successful parse ."}
{"label": "CONJUNCTION", "text": "In order to deal with ambiguity , the MORphological PArser MORPA is provided with a probabilistic context-free grammar -LRB- PCFG -RRB- , i.e. it combines a << `` conventional '' context-free morphological grammar >> to filter out ungrammatical segmentations with a [[ probability-based scoring function ]] which determines the likelihood of each successful parse ."}
{"label": "USED-FOR", "text": "In order to deal with ambiguity , the MORphological PArser MORPA is provided with a probabilistic context-free grammar -LRB- PCFG -RRB- , i.e. it combines a `` conventional '' context-free morphological grammar to filter out ungrammatical segmentations with a [[ probability-based scoring function ]] which determines the likelihood of each successful << parse >> ."}
{"label": "USED-FOR", "text": "Test performance data will show that a [[ PCFG ]] yields good results in << morphological parsing >> ."}
{"label": "HYPONYM-OF", "text": "[[ MORPA ]] is a fully implemented << parser >> developed for use in a text-to-speech conversion system ."}
{"label": "USED-FOR", "text": "[[ MORPA ]] is a fully implemented parser developed for use in a << text-to-speech conversion system >> ."}
{"label": "USED-FOR", "text": "MORPA is a fully implemented [[ parser ]] developed for use in a << text-to-speech conversion system >> ."}
{"label": "USED-FOR", "text": "This paper describes the framework of a << Korean phonological knowledge base system >> using the [[ unification-based grammar formalism ]] : Korean Phonology Structure Grammar -LRB- KPSG -RRB- ."}
{"label": "HYPONYM-OF", "text": "This paper describes the framework of a Korean phonological knowledge base system using the << unification-based grammar formalism >> : [[ Korean Phonology Structure Grammar -LRB- KPSG -RRB- ]] ."}
{"label": "USED-FOR", "text": "The [[ approach ]] of << KPSG >> provides an explicit development model for constructing a computational phonological system : speech recognition and synthesis system ."}
{"label": "USED-FOR", "text": "The approach of [[ KPSG ]] provides an explicit development model for constructing a computational << phonological system >> : speech recognition and synthesis system ."}
{"label": "COMPARE", "text": "We show that the proposed [[ approach ]] is more describable than other << approaches >> such as those employing a traditional generative phonological approach ."}
{"label": "USED-FOR", "text": "We show that the proposed approach is more describable than other approaches such as << those >> employing a traditional [[ generative phonological approach ]] ."}
{"label": "USED-FOR", "text": "In this paper , we study the [[ design of core-selecting payment rules ]] for such << domains >> ."}
{"label": "USED-FOR", "text": "We design two [[ core-selecting rules ]] that always satisfy << IR >> in expectation ."}
{"label": "USED-FOR", "text": "To study the performance of our << rules >> we perform a [[ computational Bayes-Nash equilibrium analysis ]] ."}
{"label": "COMPARE", "text": "We show that , in equilibrium , our new [[ rules ]] have better incentives , higher efficiency , and a lower rate of ex-post IR violations than standard << core-selecting rules >> ."}
{"label": "EVALUATE-FOR", "text": "We show that , in equilibrium , our new << rules >> have better incentives , higher efficiency , and a lower [[ rate of ex-post IR violations ]] than standard core-selecting rules ."}
{"label": "EVALUATE-FOR", "text": "We show that , in equilibrium , our new rules have better incentives , higher efficiency , and a lower [[ rate of ex-post IR violations ]] than standard << core-selecting rules >> ."}
{"label": "USED-FOR", "text": "In this paper , we will describe a [[ search tool ]] for a huge set of << ngrams >> ."}
{"label": "USED-FOR", "text": "This system can be a very useful [[ tool ]] for << linguistic knowledge discovery >> and other NLP tasks ."}
{"label": "USED-FOR", "text": "This system can be a very useful [[ tool ]] for linguistic knowledge discovery and other << NLP tasks >> ."}
{"label": "CONJUNCTION", "text": "This system can be a very useful tool for [[ linguistic knowledge discovery ]] and other << NLP tasks >> ."}
{"label": "PART-OF", "text": "This paper explores the role of [[ user modeling ]] in such << systems >> ."}
{"label": "USED-FOR", "text": "Since acquiring the knowledge for a [[ user model ]] is a fundamental problem in << user modeling >> , a section is devoted to this topic ."}
{"label": "PART-OF", "text": "Next , the benefits and costs of implementing a [[ user modeling component ]] for a << system >> are weighed in light of several aspects of the interaction requirements that may be imposed by the system ."}
{"label": "USED-FOR", "text": "[[ Information extraction techniques ]] automatically create << structured databases >> from unstructured data sources , such as the Web or newswire documents ."}
{"label": "USED-FOR", "text": "<< Information extraction techniques >> automatically create structured databases from [[ unstructured data sources ]] , such as the Web or newswire documents ."}
{"label": "HYPONYM-OF", "text": "Information extraction techniques automatically create structured databases from << unstructured data sources >> , such as the [[ Web ]] or newswire documents ."}
{"label": "CONJUNCTION", "text": "Information extraction techniques automatically create structured databases from unstructured data sources , such as the [[ Web ]] or << newswire documents >> ."}
{"label": "HYPONYM-OF", "text": "Information extraction techniques automatically create structured databases from << unstructured data sources >> , such as the Web or [[ newswire documents ]] ."}
{"label": "EVALUATE-FOR", "text": "Despite the successes of these << systems >> , [[ accuracy ]] will always be imperfect ."}
{"label": "USED-FOR", "text": "The << information extraction system >> we evaluate is based on a [[ linear-chain conditional random field -LRB- CRF -RRB- ]] , a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary , overlapping features of the input in a Markov model ."}
{"label": "HYPONYM-OF", "text": "The information extraction system we evaluate is based on a [[ linear-chain conditional random field -LRB- CRF -RRB- ]] , a << probabilistic model >> which has performed well on information extraction tasks because of its ability to capture arbitrary , overlapping features of the input in a Markov model ."}
{"label": "USED-FOR", "text": "The information extraction system we evaluate is based on a linear-chain conditional random field -LRB- CRF -RRB- , a [[ probabilistic model ]] which has performed well on << information extraction tasks >> because of its ability to capture arbitrary , overlapping features of the input in a Markov model ."}
{"label": "USED-FOR", "text": "The information extraction system we evaluate is based on a linear-chain conditional random field -LRB- CRF -RRB- , a [[ probabilistic model ]] which has performed well on information extraction tasks because of its ability to capture << arbitrary , overlapping features >> of the input in a Markov model ."}
{"label": "FEATURE-OF", "text": "The information extraction system we evaluate is based on a linear-chain conditional random field -LRB- CRF -RRB- , a probabilistic model which has performed well on information extraction tasks because of its ability to capture [[ arbitrary , overlapping features ]] of the << input >> in a Markov model ."}
{"label": "PART-OF", "text": "The information extraction system we evaluate is based on a linear-chain conditional random field -LRB- CRF -RRB- , a probabilistic model which has performed well on information extraction tasks because of its ability to capture [[ arbitrary , overlapping features ]] of the input in a << Markov model >> ."}
{"label": "CONJUNCTION", "text": "We implement several techniques to estimate the confidence of both [[ extracted fields ]] and entire << multi-field records >> , obtaining an average precision of 98 % for retrieving correct fields and 87 % for multi-field records ."}
{"label": "EVALUATE-FOR", "text": "We implement several << techniques >> to estimate the confidence of both extracted fields and entire multi-field records , obtaining an [[ average precision ]] of 98 % for retrieving correct fields and 87 % for multi-field records ."}
{"label": "USED-FOR", "text": "In this paper , we use the [[ information redundancy in multilingual input ]] to correct errors in << machine translation >> and thus improve the quality of multilingual summaries ."}
{"label": "USED-FOR", "text": "In this paper , we use the [[ information redundancy in multilingual input ]] to correct errors in machine translation and thus improve the quality of << multilingual summaries >> ."}
{"label": "USED-FOR", "text": "We demonstrate how errors in the << machine translations >> of the input [[ Arabic documents ]] can be corrected by identifying and generating from such redundancy , focusing on noun phrases ."}
{"label": "USED-FOR", "text": "In this paper , we propose a new [[ approach ]] to generate << oriented object proposals -LRB- OOPs -RRB- >> to reduce the detection error caused by various orientations of the object ."}
{"label": "EVALUATE-FOR", "text": "In this paper , we propose a new approach to generate << oriented object proposals -LRB- OOPs -RRB- >> to reduce the [[ detection error ]] caused by various orientations of the object ."}
{"label": "USED-FOR", "text": "To this end , we propose to efficiently locate << object regions >> according to [[ pixelwise object probability ]] , rather than measuring the objectness from a set of sampled windows ."}
{"label": "COMPARE", "text": "To this end , we propose to efficiently locate object regions according to [[ pixelwise object probability ]] , rather than measuring the << objectness >> from a set of sampled windows ."}
{"label": "USED-FOR", "text": "We formulate the << proposal generation problem >> as a [[ generative proba-bilistic model ]] such that object proposals of different shapes -LRB- i.e. , sizes and orientations -RRB- can be produced by locating the local maximum likelihoods ."}
{"label": "FEATURE-OF", "text": "We formulate the proposal generation problem as a generative proba-bilistic model such that << object proposals >> of different [[ shapes ]] -LRB- i.e. , sizes and orientations -RRB- can be produced by locating the local maximum likelihoods ."}
{"label": "HYPONYM-OF", "text": "We formulate the proposal generation problem as a generative proba-bilistic model such that object proposals of different << shapes >> -LRB- i.e. , [[ sizes ]] and orientations -RRB- can be produced by locating the local maximum likelihoods ."}
{"label": "CONJUNCTION", "text": "We formulate the proposal generation problem as a generative proba-bilistic model such that object proposals of different shapes -LRB- i.e. , [[ sizes ]] and << orientations >> -RRB- can be produced by locating the local maximum likelihoods ."}
{"label": "HYPONYM-OF", "text": "We formulate the proposal generation problem as a generative proba-bilistic model such that object proposals of different << shapes >> -LRB- i.e. , sizes and [[ orientations ]] -RRB- can be produced by locating the local maximum likelihoods ."}
{"label": "USED-FOR", "text": "We formulate the proposal generation problem as a generative proba-bilistic model such that << object proposals >> of different shapes -LRB- i.e. , sizes and orientations -RRB- can be produced by locating the [[ local maximum likelihoods ]] ."}
{"label": "USED-FOR", "text": "First , it helps the [[ object detector ]] handle objects of different << orientations >> ."}
{"label": "USED-FOR", "text": "Third , [[ it ]] avoids massive window sampling , and thereby reducing the << number of proposals >> while maintaining a high recall ."}
{"label": "EVALUATE-FOR", "text": "Third , << it >> avoids massive window sampling , and thereby reducing the number of proposals while maintaining a high [[ recall ]] ."}
{"label": "EVALUATE-FOR", "text": "Experiments on the [[ PASCAL VOC 2007 dataset ]] show that the proposed << OOP >> outperforms the state-of-the-art fast methods ."}
{"label": "COMPARE", "text": "Experiments on the PASCAL VOC 2007 dataset show that the proposed [[ OOP ]] outperforms the << state-of-the-art fast methods >> ."}
{"label": "USED-FOR", "text": "Further experiments show that the [[ rotation invariant property ]] helps a << class-specific object detector >> achieve better performance than the state-of-the-art proposal generation methods in either object rotation scenarios or general scenarios ."}
{"label": "COMPARE", "text": "Further experiments show that the rotation invariant property helps a [[ class-specific object detector ]] achieve better performance than the state-of-the-art << proposal generation methods >> in either object rotation scenarios or general scenarios ."}
{"label": "EVALUATE-FOR", "text": "Further experiments show that the rotation invariant property helps a << class-specific object detector >> achieve better performance than the state-of-the-art proposal generation methods in either [[ object rotation scenarios ]] or general scenarios ."}
{"label": "EVALUATE-FOR", "text": "Further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art << proposal generation methods >> in either [[ object rotation scenarios ]] or general scenarios ."}
{"label": "CONJUNCTION", "text": "Further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art proposal generation methods in either [[ object rotation scenarios ]] or << general scenarios >> ."}
{"label": "EVALUATE-FOR", "text": "Further experiments show that the rotation invariant property helps a << class-specific object detector >> achieve better performance than the state-of-the-art proposal generation methods in either object rotation scenarios or [[ general scenarios ]] ."}
{"label": "EVALUATE-FOR", "text": "Further experiments show that the rotation invariant property helps a class-specific object detector achieve better performance than the state-of-the-art << proposal generation methods >> in either object rotation scenarios or [[ general scenarios ]] ."}
{"label": "PART-OF", "text": "This paper describes three relatively [[ domain-independent capabilities ]] recently added to the << Paramax spoken language understanding system >> : non-monotonic reasoning , implicit reference resolution , and database query paraphrase ."}
{"label": "HYPONYM-OF", "text": "This paper describes three relatively << domain-independent capabilities >> recently added to the Paramax spoken language understanding system : [[ non-monotonic reasoning ]] , implicit reference resolution , and database query paraphrase ."}
{"label": "HYPONYM-OF", "text": "This paper describes three relatively << domain-independent capabilities >> recently added to the Paramax spoken language understanding system : non-monotonic reasoning , [[ implicit reference resolution ]] , and database query paraphrase ."}
{"label": "HYPONYM-OF", "text": "This paper describes three relatively << domain-independent capabilities >> recently added to the Paramax spoken language understanding system : non-monotonic reasoning , implicit reference resolution , and [[ database query paraphrase ]] ."}
{"label": "EVALUATE-FOR", "text": "Finally , we briefly describe an experiment which we have done in extending the << n-best speech/language integration architecture >> to improving [[ OCR accuracy ]] ."}
{"label": "USED-FOR", "text": "We investigate the problem of fine-grained sketch-based image retrieval -LRB- SBIR -RRB- , where [[ free-hand human sketches ]] are used as queries to perform << instance-level retrieval of images >> ."}
{"label": "USED-FOR", "text": "This is an extremely challenging task because -LRB- i -RRB- visual comparisons not only need to be fine-grained but also executed cross-domain , -LRB- ii -RRB- free-hand -LRB- finger -RRB- sketches are highly abstract , making fine-grained matching harder , and most importantly -LRB- iii -RRB- [[ annotated cross-domain sketch-photo datasets ]] required for training are scarce , challenging many state-of-the-art << machine learning techniques >> ."}
{"label": "USED-FOR", "text": "We then develop a [[ deep triplet-ranking model ]] for << instance-level SBIR >> with a novel data augmentation and staged pre-training strategy to alleviate the issue of insufficient fine-grained training data ."}
{"label": "USED-FOR", "text": "We then develop a [[ deep triplet-ranking model ]] for instance-level SBIR with a novel data augmentation and staged pre-training strategy to alleviate the issue of << insufficient fine-grained training data >> ."}
{"label": "USED-FOR", "text": "We then develop a << deep triplet-ranking model >> for instance-level SBIR with a novel [[ data augmentation ]] and staged pre-training strategy to alleviate the issue of insufficient fine-grained training data ."}
{"label": "CONJUNCTION", "text": "We then develop a deep triplet-ranking model for instance-level SBIR with a novel [[ data augmentation ]] and << staged pre-training strategy >> to alleviate the issue of insufficient fine-grained training data ."}
{"label": "USED-FOR", "text": "We then develop a << deep triplet-ranking model >> for instance-level SBIR with a novel data augmentation and [[ staged pre-training strategy ]] to alleviate the issue of insufficient fine-grained training data ."}
{"label": "CONJUNCTION", "text": "Extensive experiments are carried out to contribute a variety of insights into the challenges of [[ data sufficiency ]] and << over-fitting avoidance >> when training deep networks for fine-grained cross-domain ranking tasks ."}
{"label": "USED-FOR", "text": "Extensive experiments are carried out to contribute a variety of insights into the challenges of data sufficiency and over-fitting avoidance when training [[ deep networks ]] for << fine-grained cross-domain ranking tasks >> ."}
{"label": "USED-FOR", "text": "In this paper we target at generating << generic action proposals >> in [[ unconstrained videos ]] ."}
{"label": "HYPONYM-OF", "text": "Each action proposal corresponds to a << temporal series of spatial bounding boxes >> , i.e. , a [[ spatio-temporal video tube ]] , which has a good potential to locate one human action ."}
{"label": "USED-FOR", "text": "Each action proposal corresponds to a temporal series of spatial bounding boxes , i.e. , a [[ spatio-temporal video tube ]] , which has a good potential to locate one << human action >> ."}
{"label": "USED-FOR", "text": "Assuming each action is performed by a human with meaningful motion , both [[ appearance and motion cues ]] are utilized to measure the << ac-tionness >> of the video tubes ."}
{"label": "EVALUATE-FOR", "text": "Assuming each action is performed by a human with meaningful motion , both appearance and motion cues are utilized to measure the [[ ac-tionness ]] of the << video tubes >> ."}
{"label": "USED-FOR", "text": "After picking those spatiotem-poral paths of high actionness scores , our << action proposal generation >> is formulated as a [[ maximum set coverage problem ]] , where greedy search is performed to select a set of action proposals that can maximize the overall actionness score ."}
{"label": "USED-FOR", "text": "After picking those spatiotem-poral paths of high actionness scores , our action proposal generation is formulated as a maximum set coverage problem , where [[ greedy search ]] is performed to select a set of << action proposals >> that can maximize the overall actionness score ."}
{"label": "EVALUATE-FOR", "text": "After picking those spatiotem-poral paths of high actionness scores , our action proposal generation is formulated as a maximum set coverage problem , where greedy search is performed to select a set of << action proposals >> that can maximize the overall [[ actionness score ]] ."}
{"label": "COMPARE", "text": "Compared with existing [[ action proposal approaches ]] , our << action proposals >> do not rely on video segmentation and can be generated in nearly real-time ."}
{"label": "EVALUATE-FOR", "text": "Experimental results on two challenging [[ datasets ]] , MSRII and UCF 101 , validate the superior performance of our << action proposals >> as well as competitive results on action detection and search ."}
{"label": "HYPONYM-OF", "text": "Experimental results on two challenging << datasets >> , [[ MSRII ]] and UCF 101 , validate the superior performance of our action proposals as well as competitive results on action detection and search ."}
{"label": "CONJUNCTION", "text": "Experimental results on two challenging datasets , [[ MSRII ]] and << UCF 101 >> , validate the superior performance of our action proposals as well as competitive results on action detection and search ."}
{"label": "HYPONYM-OF", "text": "Experimental results on two challenging << datasets >> , MSRII and [[ UCF 101 ]] , validate the superior performance of our action proposals as well as competitive results on action detection and search ."}
{"label": "EVALUATE-FOR", "text": "Experimental results on two challenging datasets , MSRII and UCF 101 , validate the superior performance of our << action proposals >> as well as competitive results on [[ action detection and search ]] ."}
{"label": "USED-FOR", "text": "This paper reports recent research into [[ methods ]] for << creating natural language text >> ."}
{"label": "PART-OF", "text": "<< KDS -LRB- Knowledge Delivery System -RRB- >> , which embodies this [[ paradigm ]] , has distinct parts devoted to creation of the propositional units , to organization of the text , to prevention of excess redundancy , to creation of combinations of units , to evaluation of these combinations as potential sentences , to selection of the best among competing combinations , and to creation of the final text ."}
{"label": "USED-FOR", "text": "The Fragment-and-Compose paradigm and the [[ computational methods ]] of << KDS >> are described ."}
{"label": "USED-FOR", "text": "This paper explores the issue of using different [[ co-occurrence similarities ]] between terms for separating << query terms >> that are useful for retrieval from those that are harmful ."}
{"label": "USED-FOR", "text": "This paper explores the issue of using different co-occurrence similarities between terms for separating [[ query terms ]] that are useful for << retrieval >> from those that are harmful ."}
{"label": "COMPARE", "text": "This paper explores the issue of using different co-occurrence similarities between terms for separating << query terms >> that are useful for retrieval from [[ those ]] that are harmful ."}
{"label": "COMPARE", "text": "The hypothesis under examination is that [[ useful terms ]] tend to be more similar to each other than to other << query terms >> ."}
{"label": "USED-FOR", "text": "Preliminary experiments with << similarities >> computed using [[ first-order and second-order co-occurrence ]] seem to confirm the hypothesis ."}
{"label": "CONJUNCTION", "text": "We propose a new [[ phrase-based translation model ]] and << decoding algorithm >> that enables us to evaluate and compare several , previously proposed phrase-based translation models ."}
{"label": "COMPARE", "text": "Within our framework , we carry out a large number of experiments to understand better and explain why [[ phrase-based models ]] outperform << word-based models >> ."}
{"label": "HYPONYM-OF", "text": "Our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple << means >> : [[ heuristic learning of phrase translations ]] from word-based alignments and lexical weighting of phrase translations ."}
{"label": "USED-FOR", "text": "Our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple means : << heuristic learning of phrase translations >> from [[ word-based alignments ]] and lexical weighting of phrase translations ."}
{"label": "HYPONYM-OF", "text": "Our empirical results , which hold for all examined language pairs , suggest that the highest levels of performance can be obtained through relatively simple << means >> : heuristic learning of phrase translations from word-based alignments and [[ lexical weighting of phrase translations ]] ."}
{"label": "USED-FOR", "text": "Traditional [[ methods ]] for << color constancy >> can improve surface re-flectance estimates from such uncalibrated images , but their output depends significantly on the background scene ."}
{"label": "USED-FOR", "text": "Traditional [[ methods ]] for color constancy can improve << surface re-flectance estimates >> from such uncalibrated images , but their output depends significantly on the background scene ."}
{"label": "USED-FOR", "text": "Traditional methods for color constancy can improve << surface re-flectance estimates >> from such [[ uncalibrated images ]] , but their output depends significantly on the background scene ."}
{"label": "USED-FOR", "text": "We introduce the multi-view color constancy problem , and present a [[ method ]] to recover << estimates of underlying surface re-flectance >> based on joint estimation of these surface properties and the illuminants present in multiple images ."}
{"label": "USED-FOR", "text": "The [[ method ]] can exploit << image correspondences >> obtained by various alignment techniques , and we show examples based on matching local region features ."}
{"label": "USED-FOR", "text": "The method can exploit << image correspondences >> obtained by various [[ alignment techniques ]] , and we show examples based on matching local region features ."}
{"label": "USED-FOR", "text": "Our results show that [[ multi-view constraints ]] can significantly improve << estimates of both scene illuminants and object color -LRB- surface reflectance -RRB- >> when compared to a baseline single-view method ."}
{"label": "COMPARE", "text": "Our results show that << multi-view constraints >> can significantly improve estimates of both scene illuminants and object color -LRB- surface reflectance -RRB- when compared to a [[ baseline single-view method ]] ."}
{"label": "USED-FOR", "text": "Our contributions include a [[ concise , modular architecture ]] with reversible processes of << understanding >> and generation , an information-state model of reference , and flexible links between semantics and collaborative problem solving ."}
{"label": "USED-FOR", "text": "Our contributions include a [[ concise , modular architecture ]] with reversible processes of understanding and << generation >> , an information-state model of reference , and flexible links between semantics and collaborative problem solving ."}
{"label": "CONJUNCTION", "text": "Our contributions include a concise , modular architecture with reversible processes of [[ understanding ]] and << generation >> , an information-state model of reference , and flexible links between semantics and collaborative problem solving ."}